{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, How are you doing. Are we going today ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word_tokenize(words) for words in sents]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "custom_word_set = set(stopwords.words('english') + list(punctuation))\n",
    "print(custom_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = [word for word in word_tokenize(text) if word not in custom_word_set]\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "#bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(clean_words)\n",
    "sorted(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stemming --> reducing the morpholigal words to its actual/root form\n",
    "### eg, closed,closing,close --> clos\n",
    "txt = \"Mary closed on closing night when she was in the mood to close.\"\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "# tokenize the sentence\n",
    "wrds = word_tokenize(txt)\n",
    "stem_words = [st.stem(words) for words in wrds]\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parts of Speech tag (pos_tag) : which would help tag noun, verb , pronoun and so on to a list of words\n",
    "nltk.pos_tag(wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD SENSE DISAMBIGUATION --> It is the process of identifying the meaning of the word based on the context in which it occurs.\n",
    "# NLTK can help find meaning of word using the Wordnet module\n",
    "# WordNet is a lexicon and also can be considered as a thesaurus\n",
    "# Inside WordNet we have synsets which is one single meaning for a word\n",
    "# One word will have multiple meaning which will be represented by single synsets\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('none'):\n",
    "    print(f\"{ss}, {ss.definition()}\")\n",
    "    \n",
    "from nltk.wsd import lesk\n",
    "word_list = [\"Sing in the lower tone bass\",\"This sea bass is really hard to catch\"]\n",
    "#for word in word_list: print([word])\n",
    "print(\"===========================================================================\")\n",
    "count = 0\n",
    "sense_list = []\n",
    "while count < len(word_list):\n",
    "    print(count,len(word_list))\n",
    "    sense = lesk(word_tokenize(word_list[count]),\"bass\")\n",
    "    #print(sense, sense.definition())\n",
    "    sense_list.append({sense,sense.definition()})\n",
    "    count += 1\n",
    "print(sense_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoSummarizing Text --> Abstract Extraction\n",
    "# Sentence Abstraction\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "articleURL = \"https://www.thebetterindia.com/153839/delhi-govt-school-kids-communication-parents-relationships/\"\n",
    "# reading the webpage and downloading page contents\n",
    "page = urllib2.urlopen(articleURL).read().decode(\"UTF8\",\"ignore\")\n",
    "soup = BeautifulSoup(page,\"html.parser\")\n",
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"article\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" \".join(map(lambda p : p.text, soup.findAll(\"article\")))\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.encode('ascii',errors='replace').replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "custom_stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "#print(custom_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "sentk = sent_tokenize(txt)\n",
    "wordtk = word_tokenize(txt)\n",
    "#print(wordtk)\n",
    "# remove stopwords\n",
    "clean_words = [wrds for wrds in wordtk if wrds not in custom_stop_words]\n",
    "print(f\"{len(wordtk)}, {len(clean_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq = FreqDist(clean_words)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "nlargest(10,freq,key=freq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "ranking = defaultdict(int)\n",
    "#ranking\n",
    "for i,sent in enumerate(sentk):\n",
    "    for w in word_tokenize(sent.lower()):\n",
    "        if w in freq:\n",
    "            ranking[i] += freq[w]\n",
    "#ranking\n",
    "sent_idx = nlargest(3,ranking,key=ranking.get)\n",
    "[sentk[j] for j in sent_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-05-23T19:53:00-07:00&max-results=7\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-05-14T19:02:00-07:00&max-results=7&start=7&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-05-02T19:43:00-07:00&max-results=7&start=14&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-04-17T19:26:00-07:00&max-results=7&start=21&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-04-10T18:56:00-07:00&max-results=7&start=28&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-03-30T19:57:00-07:00&max-results=7&start=35&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-03-20T19:47:00-07:00&max-results=7&start=42&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-03-02T17:42:00-08:00&max-results=7&start=49&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-02-21T19:13:00-08:00&max-results=7&start=56&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-02-12T18:34:00-08:00&max-results=7&start=63&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-02-01T18:56:00-08:00&max-results=7&start=70&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-01-22T18:58:00-08:00&max-results=7&start=77&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-01-11T18:09:00-08:00&max-results=7&start=84&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2017-01-02T17:59:00-08:00&max-results=7&start=91&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-12-22T18:58:00-08:00&max-results=7&start=98&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-12-13T18:57:00-08:00&max-results=7&start=105&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-12-04T18:58:00-08:00&max-results=7&start=112&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-09-09T07:34:00-07:00&max-results=7&start=119&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-08-28T20:08:00-07:00&max-results=7&start=126&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-08-17T19:24:00-07:00&max-results=7&start=133&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-08-07T20:30:00-07:00&max-results=7&start=140&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-07-26T19:55:00-07:00&max-results=7&start=147&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-07-17T19:47:00-07:00&max-results=7&start=154&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-07-06T19:34:00-07:00&max-results=7&start=161&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-06-26T19:36:00-07:00&max-results=7&start=168&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-06-15T19:23:00-07:00&max-results=7&start=175&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-06-06T18:50:00-07:00&max-results=7&start=182&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-05-26T20:08:00-07:00&max-results=7&start=189&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-05-17T18:52:00-07:00&max-results=7&start=196&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-05-06T19:26:00-07:00&max-results=7&start=203&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-04-27T19:03:00-07:00&max-results=7&start=210&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-04-19T19:36:00-07:00&max-results=7&start=217&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-04-10T19:19:00-07:00&max-results=7&start=224&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-03-30T19:12:00-07:00&max-results=7&start=231&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-03-20T18:41:00-07:00&max-results=7&start=238&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-03-09T18:38:00-08:00&max-results=7&start=245&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-02-28T17:47:00-08:00&max-results=7&start=252&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-02-17T18:44:00-08:00&max-results=7&start=259&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-02-08T18:13:00-08:00&max-results=7&start=266&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-01-28T19:45:00-08:00&max-results=7&start=273&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-01-18T18:30:00-08:00&max-results=7&start=280&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2016-01-07T19:03:00-08:00&max-results=7&start=287&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-12-28T18:26:00-08:00&max-results=7&start=294&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-12-16T18:24:00-08:00&max-results=7&start=301&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-12-07T18:24:00-08:00&max-results=7&start=308&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-11-26T17:49:00-08:00&max-results=7&start=315&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-11-17T18:18:00-08:00&max-results=7&start=322&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-11-05T20:15:00-08:00&max-results=7&start=329&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-10-27T20:04:00-07:00&max-results=7&start=336&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-10-12T19:45:00-07:00&max-results=7&start=343&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-09-30T19:33:00-07:00&max-results=7&start=350&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-09-20T19:11:00-07:00&max-results=7&start=357&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-09-09T19:32:00-07:00&max-results=7&start=364&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-08-31T19:31:00-07:00&max-results=7&start=371&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-08-20T19:29:00-07:00&max-results=7&start=378&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-08-11T19:32:00-07:00&max-results=7&start=385&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-08-02T19:04:00-07:00&max-results=7&start=392&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-07-22T19:39:00-07:00&max-results=7&start=399&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-07-13T19:38:00-07:00&max-results=7&start=406&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-07-02T21:15:00-07:00&max-results=7&start=413&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-06-23T19:30:00-07:00&max-results=7&start=420&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-06-14T19:36:00-07:00&max-results=7&start=427&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-06-02T19:38:00-07:00&max-results=7&start=434&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-05-24T20:16:00-07:00&max-results=7&start=441&by-date=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-05-13T20:18:00-07:00&max-results=7&start=448&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-05-04T20:23:00-07:00&max-results=7&start=455&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-04-23T20:19:00-07:00&max-results=7&start=462&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-04-14T19:40:00-07:00&max-results=7&start=469&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-04-05T20:22:00-07:00&max-results=7&start=476&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-03-24T20:12:00-07:00&max-results=7&start=483&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-03-15T20:41:00-07:00&max-results=7&start=490&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-03-03T19:30:00-08:00&max-results=7&start=497&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-02-22T19:55:00-08:00&max-results=7&start=504&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-02-11T20:02:00-08:00&max-results=7&start=511&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-02-02T19:46:00-08:00&max-results=7&start=518&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-01-22T19:50:00-08:00&max-results=7&start=524&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-01-15T19:17:00-08:00&max-results=7&start=529&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2015-01-06T19:48:00-08:00&max-results=7&start=536&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2014-12-25T21:30:00-08:00&max-results=7&start=543&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2014-12-15T19:24:00-08:00&max-results=7&start=550&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2014-12-05T01:52:00-08:00&max-results=7&start=557&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2014-11-26T01:44:00-08:00&max-results=7&start=564&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2014-11-17T01:41:00-08:00&max-results=7&start=571&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2014-11-06T01:38:00-08:00&max-results=7&start=578&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2014-10-28T01:24:00-07:00&max-results=7&start=585&by-date=false\n",
      "Older Posts -- https://doxydonkey.blogspot.com/search?updated-max=2014-10-17T01:20:00-07:00&max-results=7&start=592&by-date=false\n"
     ]
    }
   ],
   "source": [
    "def getDoxyDonkeyPosts(url,links):\n",
    "    #page = urllib2.urlopen(url).read().decode(\"UTF\",\"ignore\")\n",
    "    #soup = BeautifulSoup(page,\"lxml\")\n",
    "    #return soup.prettify()\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response,\"lxml\")\n",
    "    #index = 0\n",
    "    for a in soup.findAll('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title'] \n",
    "            if title == \"Older Posts\":\n",
    "                print(f\"{title} -- {url}\")\n",
    "                links.append(url)\n",
    "                #index += 1\n",
    "                getDoxyDonkeyPosts(url,links)\n",
    "        except:\n",
    "            title = \" \"\n",
    "    return\n",
    "    \n",
    "doxyDonkeyUrl = \"https://doxydonkey.blogspot.com\"\n",
    "links = []\n",
    "getDoxyDonkeyPosts(doxyDonkeyUrl,links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doxyDonkeyText(url):\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request).read().decode(\"utf-8\",\"ignore\")\n",
    "    soup = BeautifulSoup(response,\"lxml\")\n",
    "    #print(soup)\n",
    "    mydivs = soup.findAll(\"div\",{\"class\":'post-body'})\n",
    "    #print(mydivs)\n",
    "    posts = []\n",
    "    for div in mydivs:\n",
    "        #print(\"######################### START OF DIV ###############################\")\n",
    "        #print(\"#########################################################################\")\n",
    "        #print(div)\n",
    "        #print(\"#########################################################################\")\n",
    "        #print(\"######################### END OF DIV ##################################\")\n",
    "        posts += map(lambda p:p.text , div.findAll(\"li\"))\n",
    "        #print(posts)\n",
    "    return posts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doxyDonkeyPosts = []\n",
    "for li in links:\n",
    "    #print(li)\n",
    "    doxyDonkeyPosts += doxyDonkeyText(li)\n",
    "    #print(doxyDonkeyPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')\n",
    "# Applying fit_transform() which takes list of string and returns 2-dimensional array with each row representing one document\n",
    "x = vectorizer.fit_transform(doxyDonkeyPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5266.512\n",
      "Iteration  1, inertia 2683.287\n",
      "Iteration  2, inertia 2675.887\n",
      "Iteration  3, inertia 2673.854\n",
      "Iteration  4, inertia 2673.000\n",
      "Iteration  5, inertia 2672.489\n",
      "Iteration  6, inertia 2671.919\n",
      "Iteration  7, inertia 2671.224\n",
      "Iteration  8, inertia 2670.279\n",
      "Iteration  9, inertia 2669.322\n",
      "Iteration 10, inertia 2668.693\n",
      "Iteration 11, inertia 2668.335\n",
      "Iteration 12, inertia 2668.079\n",
      "Iteration 13, inertia 2667.956\n",
      "Iteration 14, inertia 2667.871\n",
      "Iteration 15, inertia 2667.810\n",
      "Iteration 16, inertia 2667.750\n",
      "Iteration 17, inertia 2667.703\n",
      "Iteration 18, inertia 2667.676\n",
      "Iteration 19, inertia 2667.665\n",
      "Iteration 20, inertia 2667.662\n",
      "Iteration 21, inertia 2667.659\n",
      "Converged at iteration 21: center shift 0.000000e+00 within tolerance 7.308517e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=3, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2], dtype=int32), array([1668,  736,  400]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = {}\n",
    "\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    oneDoucument = doxyDonkeyPosts[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDoucument\n",
    "    else:\n",
    "        text[cluster] += oneDoucument\n",
    "#print(text)\n",
    "#print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most frequent words in the cluster excluding the stopwords\n",
    "_stopwords = set(stopwords.words('english') + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "count = {}\n",
    "for cluster in range(3):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent = [word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100,freq, key=freq.get)\n",
    "    count[cluster] = freq\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['uber', 'india', 'tech', 'chinese', 'deal', 'capital', 'money', 'public', 'round', 'firm'], 1: ['ads', 'video', 'ad', 'apps', 'search', 'social', 'product', 'buy', 'snapchat', 'youtube'], 2: ['quarter', 'share', 'profit', 'rose', 'earnings', 'analysts', 'cents', 'net', 'fell', 'reported']}\n"
     ]
    }
   ],
   "source": [
    "unique_keys = {}\n",
    "for cluster in range(3):\n",
    "    \n",
    "    other_clusters = list( set(range(3)) - set([cluster]))\n",
    "    key_other_clusters = set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique = set(keywords[cluster]) - key_other_clusters\n",
    "    unique_keys[cluster] = nlargest(10,unique,key=count[cluster].get)\n",
    "                          \n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Apple has acquired music analytics start-up Asaii, Axios reported on Sunday, citing sources, in a bid to strengthen content recommendations to its music users.Apple's deal to buy Asaii was worth less than $100 million, Axios reported, citing a source.Asaii can help Apple compete with Spotify Technology SA's efforts to work directly with smaller artists, like a music label, the report added.Last month, Apple completed the acquisition of music discovery app Shazam, a deal to help it better compete with Spotify, the industry leader in music streaming services.Apple and Asaii were not immediately available for comment outside regular business hours.\"\n",
    "\n",
    "### Classification using KNN clustering algorithm\n",
    "from sklearn. import KNeighboursClassifier\n",
    "classifier = KNeighboursClassifer()\n",
    "classifier(x,km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x13219 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 52 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = vectorizer.transform([article])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predit(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install sklearn\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Packages\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probabilty import FreqDist\n",
    "from collections import defaultdict\n",
    "from headq import nlargest\n",
    "from string import punctuation\n",
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.classifier import KNeighourClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to get post from an webpage\n",
    "## Data Extraction phase\n",
    "\n",
    "################################\n",
    "### Start of Data Extraction ###\n",
    "################################\n",
    "\n",
    "# 0. Creating a Soup object\n",
    "def easySoup(url,parser=\"lxml\"):\n",
    "    request = urllib.Request(url)\n",
    "    response = urllib.urlopen(request)\n",
    "    soup = BeautifulSoup(response,parser)\n",
    "    return soup\n",
    "\n",
    "# 1. Fetching all the links contained in the page \n",
    "def getAllLinksFromPage(url):\n",
    "    # empty list\n",
    "    links = []\n",
    "    #request = urllib.Request(url)\n",
    "    #response = urllib.urlopen(request)\n",
    "    #soup = BeautifulSoup(response,\"lxml\")\n",
    "    soup = easySoup(url)\n",
    "    # Iterating through page using soup object and getting all the links as LIST\n",
    "    for a in soup.findAll(\"a\"):\n",
    "        # Handling Exception\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title']\n",
    "            if title == \"Older Posts\":\n",
    "                links.append(url)\n",
    "                # Making a recursive function call\n",
    "                getAllLinksFromPage(url,links)\n",
    "        except:\n",
    "            title = \"\"\n",
    "    return links\n",
    "\n",
    "\n",
    "# 2. Getting the content from the links which we obtained using getAllLinksFromPage(url,links)\n",
    "def getAllPostsFromPage(links):\n",
    "    soup = easySoup(links)\n",
    "    posts = []\n",
    "    for link in soup.findAll('div',{\"class\":\"post-body\"}):\n",
    "        posts += map(lambda p:p.text, link.findAll('li') )        \n",
    "    return posts\n",
    "\n",
    "\n",
    "# 3. Iterate through the list of links and pass it to getAllPostsFromPage(links) function to extract the page content\n",
    "everyLinkPossiblleInPage = getAllLinksFromPage(url)\n",
    "everySinglePostInPage = []\n",
    "for aLink in everyLinkPossiblle:\n",
    "    # Appends all the post from each link into everSinglePostInPage List\n",
    "    everySinglePostInPage += getAllPostsFromPage(aLink)\n",
    "    \n",
    "################################\n",
    "### End of Data Extraction ###\n",
    "################################\n",
    "\n",
    "\n",
    "## Feature Extraction Phase\n",
    "\n",
    "###################################\n",
    "### Start of Feature Extraction ###\n",
    "###################################\n",
    "\n",
    "def f_extract(docs):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "    # Applying fit_transform() which takes list of string and returns\n",
    "    # 2-dimensional array with each row representing one document\n",
    "    features = vectorizer.fit_transform(docs)\n",
    "    return features\n",
    "\n",
    "doc_features = f_extract(everySinglePostInPage)\n",
    "###################################\n",
    "### End of Feature Extraction ###\n",
    "###################################\n",
    "\n",
    "\n",
    "## Clustering the features using KMeans Algm\n",
    "\n",
    "def getCluster(features):\n",
    "    word_cluster = KMeans(n_clusters=3, init='kmeans++', max_iter=100, n_init=1, verbose=True)\n",
    "    return word_cluster.fit(features)\n",
    "\n",
    "word_cluster = getCluster(doc_features)\n",
    "\n",
    "# To see how many clusters the document has been split\n",
    "np.unique(word_cluster.labels_, count=True)\n",
    "\n",
    "# Indexing the cluster with posts\n",
    "def cluster_with_post(cluster):\n",
    "    text = {}\n",
    "    for i,cluster in enumerate(cluster):\n",
    "        oneDocument = everySinglePostInPage[i]\n",
    "        if cluster not in text.keys():\n",
    "            text[cluster] = oneDocument\n",
    "        else:\n",
    "            text[cluster] += oneDocument\n",
    "    return text\n",
    "\n",
    "document_dict = cluster_with_post(word_cluster.labels_)\n",
    "\n",
    "\n",
    "\n",
    "# Creating a set of custom stopwords\n",
    "_stopwords = set(stopwords.word('english') + list(punctuations))\n",
    "\n",
    "# extract keywords and count of keywords in each cluster as dict\n",
    "\n",
    "keywords = {}\n",
    "count = {}\n",
    "for cluster in range(3):\n",
    "    # tokenzing the words in cluster\n",
    "    tok_word = word_tokenize(document_dict[cluster].lower())\n",
    "    # removing the not so important words\n",
    "    tok_word = [word for word in tok_word if word not in _stopwords]\n",
    "    # Consturcting the Frequcy distribution table. This gives frequency of each word\n",
    "    freq_dist = FreqDist(tok_word)\n",
    "    # Storing the above details into dict\n",
    "    keywords[cluster] = nlargest(100, freq_dist, key=freq_dist.get)\n",
    "    count[cluster] = freq_dict\n",
    "    \n",
    "# Getting all the unique keys from each cluster\n",
    "unique_keys = {}\n",
    "for cluster in range(3):\n",
    "    # Getting a other cluster index values by subtracting the current cluster index value with the others\n",
    "    other_cluster = list(set(range(3))-set([cluster]))\n",
    "    # Joining the keywords from other clusters and making a set\n",
    "    keywords_from_other_cluster = set(keywords[other_cluster[0]]).union(set(keywords[other_cluster[1]]))\n",
    "    # Finally eliminating all the keywords from other cluster which is present in current cluster,\n",
    "    # thus getting unique keys from each cluster\n",
    "    u_keys = set(keywords[cluster]) - keywords_from_other_cluster\n",
    "    # Unique keys dict is made with cluster index and the 10 uniquie_keys\n",
    "    unique_keys[cluster] = nlargest(10,u_keys, key=count[cluster].get)\n",
    "\n",
    "       \n",
    "## Classifying the document using the KNeighbours Algm\n",
    "classifier = KNeighbourClassifier()\n",
    "classifier(doc_features,word_cluster.labels_)\n",
    "\n",
    "# To test a doc\n",
    "# convet the doc into vector and then use classifier to pridict\n",
    "# eg : test = vectorizer([document])\n",
    "# classifier.predict(test)\n",
    "vectorizer = TfidfVecotrizer(max_df=0.5,min_df=2,stop_words='english')\n",
    "test = vecotrizer.transform(doc)\n",
    "classifier.predict(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
